{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch13_04.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabumjung/Machine-Learning-Algorithm/blob/master/ch13_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ql2olOyqcycb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#확률적 잠재 의미 분석"
      ]
    },
    {
      "metadata": {
        "id": "g8vR0UfxYcCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import brown\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqHcUpC6dOdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # For reproducibility\n",
        "np.random.seed(1000)\n",
        "\n",
        "rank = 2\n",
        "alpha_1 = 1000.0\n",
        "alpha_2 = 10.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ngK_kx0bfmbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compose a corpus\n",
        "sentences_1 = brown.sents(categories=['editorial'])[0:20]\n",
        "sentences_2 = brown.sents(categories=['fiction'])[0:20]\n",
        "corpus = []\n",
        "\n",
        "for s in sentences_1 + sentences_2:\n",
        "    corpus.append(' '.join(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMN4_bb2gA_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vectorize the corpus\n",
        "cv = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
        "Xc = np.array(cv.fit_transform(corpus).todense())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4emrIEHZgDTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the probability matrices\n",
        "Ptd = np.random.uniform(0.0, 1.0, size=(len(corpus), rank))\n",
        "Pwt = np.random.uniform(0.0, 1.0, size=(rank, len(cv.vocabulary_)))\n",
        "Ptdw = np.zeros(shape=(len(cv.vocabulary_), len(corpus), rank))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OusX5szWgFeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize the probability matrices\n",
        "for d in range(len(corpus)):\n",
        "    nf = np.sum(Ptd[d, :])\n",
        "    for t in range(rank):\n",
        "        Ptd[d, t] /= nf\n",
        "\n",
        "for t in range(rank):\n",
        "    nf = np.sum(Pwt[t, :])\n",
        "    for w in range(len(cv.vocabulary_)):\n",
        "        Pwt[t, w] /= nf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMVjQxgOgHuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_likelihood():\n",
        "    value = 0.0\n",
        "\n",
        "    for d in range(len(corpus)):\n",
        "        for w in range(len(cv.vocabulary_)):\n",
        "            real_topic_value = 0.0\n",
        "\n",
        "            for t in range(rank):\n",
        "                real_topic_value += Ptd[d, t] * Pwt[t, w]\n",
        "\n",
        "            if real_topic_value > 0.0:\n",
        "                value += Xc[d, w] * np.log(real_topic_value)\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "def expectation():\n",
        "    global Ptd, Pwt, Ptdw\n",
        "\n",
        "    for d in range(len(corpus)):\n",
        "        for w in range(len(cv.vocabulary_)):\n",
        "            nf = 0.0\n",
        "\n",
        "            for t in range(rank):\n",
        "                Ptdw[w, d, t] = Ptd[d, t] * Pwt[t, w]\n",
        "                nf += Ptdw[w, d, t]\n",
        "\n",
        "            Ptdw[w, d, :] = (Ptdw[w, d, :] / nf) if nf != 0.0 else 0.0\n",
        "\n",
        "\n",
        "def maximization():\n",
        "    global Ptd, Pwt, Ptdw\n",
        "\n",
        "    for t in range(rank):\n",
        "        nf = 0.0\n",
        "\n",
        "        for d in range(len(corpus)):\n",
        "            ps = 0.0\n",
        "\n",
        "            for w in range(len(cv.vocabulary_)):\n",
        "                ps += Xc[d, w] * Ptdw[w, d, t]\n",
        "\n",
        "            Pwt[t, w] = ps\n",
        "            nf += Pwt[t, w]\n",
        "\n",
        "        Pwt[:, w] /= nf if nf != 0.0 else alpha_1\n",
        "\n",
        "    for d in range(len(corpus)):\n",
        "        for t in range(rank):\n",
        "            ps = 0.0\n",
        "            nf = 0.0\n",
        "\n",
        "            for w in range(len(cv.vocabulary_)):\n",
        "                ps += Xc[d, w] * Ptdw[w, d, t]\n",
        "                nf += Xc[d, w]\n",
        "\n",
        "            Ptd[d, t] = ps / (nf if nf != 0.0 else alpha_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Unj4ImYNgKTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "4c134351-9386-43d2-cdfc-f0622ae3f432"
      },
      "cell_type": "code",
      "source": [
        "print('Initial Log-Likelihood: %f' % log_likelihood())\n",
        "\n",
        "for i in range(30):\n",
        "    expectation()\n",
        "    maximization()\n",
        "    print('Step %d - Log-Likelihood: %f' % (i, log_likelihood()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Log-Likelihood: -2380.516058\n",
            "Step 0 - Log-Likelihood: -2375.232302\n",
            "Step 1 - Log-Likelihood: -2369.644941\n",
            "Step 2 - Log-Likelihood: -2366.693462\n",
            "Step 3 - Log-Likelihood: -2365.264721\n",
            "Step 4 - Log-Likelihood: -2364.678040\n",
            "Step 5 - Log-Likelihood: -2364.480283\n",
            "Step 6 - Log-Likelihood: -2364.372574\n",
            "Step 7 - Log-Likelihood: -2364.217400\n",
            "Step 8 - Log-Likelihood: -2364.015236\n",
            "Step 9 - Log-Likelihood: -2363.816327\n",
            "Step 10 - Log-Likelihood: -2363.652013\n",
            "Step 11 - Log-Likelihood: -2363.525275\n",
            "Step 12 - Log-Likelihood: -2363.427930\n",
            "Step 13 - Log-Likelihood: -2363.351643\n",
            "Step 14 - Log-Likelihood: -2363.290491\n",
            "Step 15 - Log-Likelihood: -2363.240594\n",
            "Step 16 - Log-Likelihood: -2363.199369\n",
            "Step 17 - Log-Likelihood: -2363.165010\n",
            "Step 18 - Log-Likelihood: -2363.136188\n",
            "Step 19 - Log-Likelihood: -2363.111882\n",
            "Step 20 - Log-Likelihood: -2363.091288\n",
            "Step 21 - Log-Likelihood: -2363.073763\n",
            "Step 22 - Log-Likelihood: -2363.058786\n",
            "Step 23 - Log-Likelihood: -2363.045932\n",
            "Step 24 - Log-Likelihood: -2363.034854\n",
            "Step 25 - Log-Likelihood: -2363.025269\n",
            "Step 26 - Log-Likelihood: -2363.016942\n",
            "Step 27 - Log-Likelihood: -2363.009679\n",
            "Step 28 - Log-Likelihood: -2363.003321\n",
            "Step 29 - Log-Likelihood: -2362.997734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LaE-xtHmgOPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "96cc1a68-5f8b-49c7-f1df-16e809977ee7"
      },
      "cell_type": "code",
      "source": [
        "# Show the top 5 words per topic\n",
        "Pwts = np.argsort(Pwt, axis=1)[::-1]\n",
        "\n",
        "for t in range(rank):\n",
        "    print('\\nTopic ' + str(t))\n",
        "    for i in range(5):\n",
        "        print(cv.get_feature_names()[Pwts[t, i]])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 0\n",
            "years\n",
            "questions\n",
            "south\n",
            "reform\n",
            "social\n",
            "\n",
            "Topic 1\n",
            "convened\n",
            "maintenance\n",
            "penal\n",
            "year\n",
            "legislators\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vyhYA37kgZ1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}