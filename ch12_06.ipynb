{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch12_06.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabumjung/Machine-Learning-Algorithm/blob/master/ch12_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2PIhVcxPVtob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TL_gecJnYgQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For reproducibility\n",
        "np.random.seed(1000)\n",
        "\n",
        "ret = RegexpTokenizer('[a-zA-Z0-9\\']+')\n",
        "sw = set(stopwords.words('english'))\n",
        "ess = SnowballStemmer('english', ignore_stopwords=True)\n",
        "\n",
        "\n",
        "def tokenizer(sentence):\n",
        "    tokens = ret.tokenize(sentence)\n",
        "    return [ess.stem(t) for t in tokens if t not in sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6Rm0R-xYBc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "e4345fbd-db08-433f-a674-ac9065e0484c"
      },
      "cell_type": "code",
      "source": [
        "# Create a corpus\n",
        "corpus = [\n",
        "    'This is a simple test corpus',\n",
        "    'A corpus is a set of text documents',\n",
        "    'We want to analyze the corpus and the documents',\n",
        "    'Documents can be automatically tokenized'\n",
        "]\n",
        "\n",
        "# Create a count vectorizer\n",
        "print('Count vectorizer:')\n",
        "cv = CountVectorizer()\n",
        "\n",
        "vectorized_corpus = cv.fit_transform(corpus)\n",
        "print(vectorized_corpus.todense())\n",
        "\n",
        "print('CV Vocabulary:')\n",
        "print(cv.vocabulary_)\n",
        "\n",
        "# Perform an inverse transformation\n",
        "vector = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n",
        "print(cv.inverse_transform(vector))\n",
        "\n",
        "# Use a complete external tokenizer\n",
        "print('CV with external tokenizer:')\n",
        "cv = CountVectorizer(tokenizer=tokenizer)\n",
        "vectorized_corpus = cv.fit_transform(corpus)\n",
        "print(vectorized_corpus.todense())\n",
        "\n",
        "# Use an n-gram range equal to (1, 2)\n",
        "print('CV witn n-gram range (1, 2):')\n",
        "cv = CountVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))\n",
        "vectorized_corpus = cv.fit_transform(corpus)\n",
        "print(vectorized_corpus.todense())\n",
        "\n",
        "print('N-gram range (1,2) vocabulary:')\n",
        "print(cv.vocabulary_)\n",
        "\n",
        "# Create a Tf-Idf vectorizer\n",
        "print('Tf-Idf vectorizer:')\n",
        "tfidfv = TfidfVectorizer()\n",
        "vectorized_corpus = tfidfv.fit_transform(corpus)\n",
        "print(vectorized_corpus.todense())\n",
        "\n",
        "print('Tf-Idf vocabulary:')\n",
        "print(tfidfv.vocabulary_)\n",
        "\n",
        "# Use n-gram range equal to (1, 2) and L2 normalization\n",
        "print('Tf-Idf witn n-gram range (1, 2) and L2 normalization:')\n",
        "tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')\n",
        "vectorized_corpus = tfidfv.fit_transform(corpus)\n",
        "print(vectorized_corpus.todense())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count vectorizer:\n",
            "[[0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 1 1 0 0 0 0 0 0 2 0 1 0 1 1]\n",
            " [0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
            "CV Vocabulary:\n",
            "{'this': 14, 'is': 7, 'simple': 10, 'test': 11, 'corpus': 5, 'set': 9, 'of': 8, 'text': 12, 'documents': 6, 'we': 18, 'want': 17, 'to': 15, 'analyze': 0, 'the': 13, 'and': 1, 'can': 4, 'be': 3, 'automatically': 2, 'tokenized': 16}\n",
            "[array(['corpus', 'is', 'simple', 'test', 'this', 'want', 'we'],\n",
            "      dtype='<U13')]\n",
            "CV with external tokenizer:\n",
            "[[0 0 1 0 0 1 1 0 0 0]\n",
            " [0 0 1 1 1 0 0 1 0 0]\n",
            " [1 0 1 1 0 0 0 0 0 1]\n",
            " [0 1 0 1 0 0 0 0 1 0]]\n",
            "CV witn n-gram range (1, 2):\n",
            "[[0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0]\n",
            " [1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
            " [0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0]]\n",
            "N-gram range (1,2) vocabulary:\n",
            "{'simpl': 11, 'test': 13, 'corpus': 4, 'simpl test': 12, 'test corpus': 14, 'set': 9, 'text': 15, 'document': 7, 'corpus set': 6, 'set text': 10, 'text document': 16, 'want': 18, 'analyz': 0, 'want analyz': 19, 'analyz corpus': 1, 'corpus document': 5, 'automat': 2, 'token': 17, 'document automat': 8, 'automat token': 3}\n",
            "Tf-Idf vectorizer:\n",
            "[[0.         0.         0.         0.         0.         0.31799276\n",
            "  0.         0.39278432 0.         0.         0.49819711 0.49819711\n",
            "  0.         0.         0.49819711 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.30304005\n",
            "  0.30304005 0.37431475 0.4747708  0.4747708  0.         0.\n",
            "  0.4747708  0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.31919701 0.31919701 0.         0.         0.         0.20373932\n",
            "  0.20373932 0.         0.         0.         0.         0.\n",
            "  0.         0.63839402 0.         0.31919701 0.         0.31919701\n",
            "  0.31919701]\n",
            " [0.         0.         0.47633035 0.47633035 0.47633035 0.\n",
            "  0.30403549 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.47633035 0.\n",
            "  0.        ]]\n",
            "Tf-Idf vocabulary:\n",
            "{'this': 14, 'is': 7, 'simple': 10, 'test': 11, 'corpus': 5, 'set': 9, 'of': 8, 'text': 12, 'documents': 6, 'we': 18, 'want': 17, 'to': 15, 'analyze': 0, 'the': 13, 'and': 1, 'can': 4, 'be': 3, 'automatically': 2, 'tokenized': 16}\n",
            "Tf-Idf witn n-gram range (1, 2) and L2 normalization:\n",
            "[[0.         0.         0.         0.         0.30403549 0.\n",
            "  0.         0.         0.         0.         0.         0.47633035\n",
            "  0.47633035 0.47633035 0.47633035 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.2646963  0.\n",
            "  0.4146979  0.2646963  0.         0.4146979  0.4146979  0.\n",
            "  0.         0.         0.         0.4146979  0.4146979  0.\n",
            "  0.         0.        ]\n",
            " [0.4146979  0.4146979  0.         0.         0.2646963  0.4146979\n",
            "  0.         0.2646963  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.4146979  0.4146979 ]\n",
            " [0.         0.         0.47633035 0.47633035 0.         0.\n",
            "  0.         0.30403549 0.47633035 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.47633035\n",
            "  0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8vR0UfxYcCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}